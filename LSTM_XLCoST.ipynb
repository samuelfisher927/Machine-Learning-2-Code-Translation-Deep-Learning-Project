{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELzXHi22SMQ1"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6LHDjU4FRJ1a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths to tokenized XLCoST files\n",
        "train_py_file = \"train-Python-Javascript-tok.py\"\n",
        "train_js_file = \"train-Python-Javascript-tok.js\"\n",
        "val_py_file = \"val-Python-Javascript-tok.py\"\n",
        "val_js_file = \"val-Python-Javascript-tok.js\"\n",
        "\n",
        "# Load tokenized lines (space-separated tokens per line)\n",
        "def load_tokenized_code(py_path, js_path):\n",
        "    with open(py_path, encoding='utf-8') as f_py, open(js_path, encoding='utf-8') as f_js:\n",
        "        py_lines = [line.strip().split() for line in f_py if line.strip()]\n",
        "        js_lines = [line.strip().split() for line in f_js if line.strip()]\n",
        "    assert len(py_lines) == len(js_lines), \"Mismatch between Python and JavaScript lines\"\n",
        "    return list(zip(py_lines, js_lines))\n",
        "\n",
        "train_pairs = load_tokenized_code(train_py_file, train_js_file)\n",
        "val_pairs   = load_tokenized_code(val_py_file, val_js_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_icTvzJRagn"
      },
      "source": [
        "## Data Preprocesing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AhbAKXKWRX8Y"
      },
      "outputs": [],
      "source": [
        "SPECIAL_TOKENS = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "\n",
        "def build_vocab(token_lists, max_vocab_size=10000):\n",
        "    all_tokens = list(chain.from_iterable(token_lists))\n",
        "    token_counts = Counter(all_tokens)\n",
        "    most_common = token_counts.most_common(max_vocab_size - len(SPECIAL_TOKENS))\n",
        "    vocab = SPECIAL_TOKENS + [token for token, _ in most_common]\n",
        "    token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "    id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
        "    return token_to_id, id_to_token\n",
        "\n",
        "def tokens_to_ids(tokens, token_to_id, add_sos=False, add_eos=False):\n",
        "    ids = [token_to_id.get(token, token_to_id['<unk>']) for token in tokens]\n",
        "    if add_sos:\n",
        "        ids = [token_to_id['<sos>']] + ids\n",
        "    if add_eos:\n",
        "        ids += [token_to_id['<eos>']]\n",
        "    return ids\n",
        "\n",
        "py_token_to_id, py_id_to_token = build_vocab([py for py, _ in train_pairs])\n",
        "js_token_to_id, js_id_to_token = build_vocab([js for _, js in train_pairs])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dmgP3pjRePY"
      },
      "source": [
        "### Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nOaRdXRwRhf0"
      },
      "outputs": [],
      "source": [
        "class CodeTranslationDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.encoder_inputs = df['py_tensor'].tolist()     # Python input\n",
        "        self.decoder_inputs = df['js_in_tensor'].tolist()  # JavaScript input (with <sos>)\n",
        "        self.decoder_outputs = df['js_out_tensor'].tolist()  # JavaScript output (with <eos>)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoder_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoder_inputs[idx], self.decoder_inputs[idx], self.decoder_outputs[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    enc_inputs, dec_inputs, dec_outputs = zip(*batch)\n",
        "    enc_inputs_pad = pad_sequence(enc_inputs, batch_first=True, padding_value=py_token_to_id['<pad>'])\n",
        "    dec_inputs_pad = pad_sequence(dec_inputs, batch_first=True, padding_value=js_token_to_id['<pad>'])\n",
        "    dec_outputs_pad = pad_sequence(dec_outputs, batch_first=True, padding_value=js_token_to_id['<pad>'])\n",
        "    return enc_inputs_pad, dec_inputs_pad, dec_outputs_pad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6CZF85rRmZS"
      },
      "source": [
        "### Attention-based LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZgQdgcgBRlpT"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = torch.sum(self.v * energy, dim=2)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(hid_dim + emb_dim, hid_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hid_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.attn = Attention(hid_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell, encoder_outputs):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attn(hidden[-1], encoder_outputs).unsqueeze(1)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
        "        prediction = self.fc_out(torch.cat((output.squeeze(1), weighted.squeeze(1)), dim=1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "\n",
        "class AttnEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "\n",
        "class AttnSeq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "        trg_len = trg.size(1)\n",
        "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8y0pgtUOvX"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "aVDfqN72UZwQ"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "\n",
        "    def step(self, val_loss):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, clip, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for src, trg_in, trg_out in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        src, trg_in, trg_out = src.to(DEVICE), trg_in.to(DEVICE), trg_out.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Pass the teacher forcing ratio to the model\n",
        "        output = model(src, trg_in, teacher_forcing_ratio)\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg_out = trg_out[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, trg_out)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg_in, trg_out in dataloader:\n",
        "            src, trg_in, trg_out = src.to(DEVICE), trg_in.to(DEVICE), trg_out.to(DEVICE)\n",
        "            output = model(src, trg_in, teacher_forcing_ratio=0.0)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg_out = trg_out[:, 1:].reshape(-1)\n",
        "            loss = criterion(output, trg_out)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGRFFuShZseB"
      },
      "source": [
        "### Beam Search & BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "960B8g9MUZ6i"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, src_tensor, beam_width=3, max_len=300):\n",
        "    model.eval()\n",
        "    src_tensor = src_tensor.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "    # Start with <sos> token\n",
        "    sequences = [[[], 0.0, torch.tensor([js_token_to_id['<sos>']], device=DEVICE), hidden, cell]]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        all_candidates = []\n",
        "        for seq, score, last_token, hidden, cell in sequences:\n",
        "            with torch.no_grad():\n",
        "                output, hidden, cell = model.decoder(last_token, hidden, cell, encoder_outputs)\n",
        "\n",
        "            probs = torch.log_softmax(output, dim=1)\n",
        "            topk = torch.topk(probs, beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                token = topk.indices[0][i]\n",
        "                prob = topk.values[0][i].item()\n",
        "                candidate = [seq + [token.item()], score + prob, token.unsqueeze(0), hidden, cell]\n",
        "                all_candidates.append(candidate)\n",
        "\n",
        "        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        if all(seq[0][-1] == js_token_to_id['<eos>'] for seq in sequences):\n",
        "            break\n",
        "\n",
        "    return [js_id_to_token.get(idx, '<unk>') for idx in sequences[0][0] if idx != js_token_to_id['<eos>']]\n",
        "\n",
        "\n",
        "def compute_bleu(pred_tokens, ref_tokens):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "\n",
        "def evaluate_bleu(model, val_df, num_samples=5):\n",
        "    total_bleu = 0\n",
        "    for i in range(min(num_samples, len(val_df))):\n",
        "        row = val_df.iloc[i]\n",
        "        src_tensor = torch.tensor(row['py_input_ids'], dtype=torch.long)\n",
        "        pred = beam_search(model, src_tensor)\n",
        "        ref = row['js_tokens']\n",
        "        bleu = compute_bleu(pred, ref)\n",
        "        total_bleu += bleu\n",
        "        print(f\"Example {i+1} BLEU: {bleu:.4f}\")\n",
        "    avg_bleu = total_bleu / num_samples\n",
        "    print(f\"\\nAvg BLEU over {num_samples} samples: {avg_bleu:.4f}\")\n",
        "    return avg_bleu\n",
        "\n",
        "def save_model(model, path=\"best_model.pt\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, path=\"best_model.pt\"):\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96f5w3vaB_9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kPgME7nfaFoC"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "def make_tensor_dataset(pairs, src_vocab, tgt_vocab, max_len=300):\n",
        "    data = []\n",
        "    for src_tokens, tgt_tokens in pairs:\n",
        "        src_ids = tokens_to_ids(src_tokens, src_vocab)\n",
        "        tgt_in_ids = tokens_to_ids(tgt_tokens, tgt_vocab, add_sos=True)\n",
        "        tgt_out_ids = tokens_to_ids(tgt_tokens, tgt_vocab, add_eos=True)\n",
        "\n",
        "        # Skip overly long examples\n",
        "        if len(src_ids) > max_len or len(tgt_in_ids) > max_len or len(tgt_out_ids) > max_len:\n",
        "            continue\n",
        "\n",
        "        data.append({\n",
        "            \"py_tensor\": torch.tensor(src_ids, dtype=torch.long),\n",
        "            \"js_in_tensor\": torch.tensor(tgt_in_ids, dtype=torch.long),\n",
        "            \"js_out_tensor\": torch.tensor(tgt_out_ids, dtype=torch.long),\n",
        "            \"py_input_ids\": src_ids,     # for beam search\n",
        "            \"js_tokens\": tgt_tokens      # for BLEU evaluation\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Convert the XLCoST pairs into DataFrames\n",
        "train_df = make_tensor_dataset(train_pairs, py_token_to_id, js_token_to_id)\n",
        "val_df   = make_tensor_dataset(val_pairs, py_token_to_id, js_token_to_id)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train-validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CtF1vOI6aR9H"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(CodeTranslationDataset(train_df), batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(CodeTranslationDataset(val_df), batch_size=1, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2mg3jL8xaSHC"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "INPUT_DIM = len(py_token_to_id)\n",
        "OUTPUT_DIM = len(js_token_to_id)\n",
        "EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "\n",
        "# Use the LSTM-based encoder and decoder defined earlier\n",
        "encoder = AttnEncoder(INPUT_DIM, EMB_DIM, HID_DIM).to(DEVICE)\n",
        "decoder = AttnDecoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(DEVICE)\n",
        "model = AttnSeq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=js_token_to_id['<pad>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GL8HjLJVb1Tw"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.0337\n",
            "Example 2 BLEU: 0.0052\n",
            "Example 3 BLEU: 0.8155\n",
            "Example 4 BLEU: 0.3605\n",
            "Example 5 BLEU: 0.7286\n",
            "\n",
            "Avg BLEU over 5 samples: 0.3887\n",
            "Epoch 1 | Train Loss: 1.7279 | Val Loss: 2.2130 | BLEU: 0.3887\n",
            " Best model saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.0444\n",
            "Example 2 BLEU: 0.1642\n",
            "Example 3 BLEU: 0.8155\n",
            "Example 4 BLEU: 0.9122\n",
            "Example 5 BLEU: 0.0688\n",
            "\n",
            "Avg BLEU over 5 samples: 0.4010\n",
            "Epoch 2 | Train Loss: 1.3909 | Val Loss: 2.1552 | BLEU: 0.4010\n",
            " Best model saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.0438\n",
            "Example 2 BLEU: 0.1642\n",
            "Example 3 BLEU: 0.6263\n",
            "Example 4 BLEU: 0.9200\n",
            "Example 5 BLEU: 0.9311\n",
            "\n",
            "Avg BLEU over 5 samples: 0.5371\n",
            "Epoch 3 | Train Loss: 1.3015 | Val Loss: 2.1880 | BLEU: 0.5371\n",
            " Best model saved\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.0796\n",
            "Example 2 BLEU: 0.0945\n",
            "Example 3 BLEU: 0.8155\n",
            "Example 4 BLEU: 0.9200\n",
            "Example 5 BLEU: 0.5393\n",
            "\n",
            "Avg BLEU over 5 samples: 0.4898\n",
            "Epoch 4 | Train Loss: 1.2472 | Val Loss: 2.1961 | BLEU: 0.4898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.0530\n",
            "Example 2 BLEU: 0.1553\n",
            "Example 3 BLEU: 0.8155\n",
            "Example 4 BLEU: 0.7539\n",
            "Example 5 BLEU: 0.7368\n",
            "\n",
            "Avg BLEU over 5 samples: 0.5029\n",
            "Epoch 5 | Train Loss: 1.2205 | Val Loss: 2.2902 | BLEU: 0.5029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.0841\n",
            "Example 2 BLEU: 0.1642\n",
            "Example 3 BLEU: 0.8155\n",
            "Example 4 BLEU: 0.9200\n",
            "Example 5 BLEU: 0.4903\n",
            "\n",
            "Avg BLEU over 5 samples: 0.4948\n",
            "Epoch 6 | Train Loss: 1.2054 | Val Loss: 2.2300 | BLEU: 0.4948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example 1 BLEU: 0.2234\n",
            "Example 2 BLEU: 0.1642\n",
            "Example 3 BLEU: 0.8155\n",
            "Example 4 BLEU: 0.9122\n",
            "Example 5 BLEU: 0.5533\n",
            "\n",
            "Avg BLEU over 5 samples: 0.5337\n",
            "Epoch 7 | Train Loss: 1.1966 | Val Loss: 2.2832 | BLEU: 0.5337\n",
            " Early stopping triggered.\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 30\n",
        "CLIP = 1.0\n",
        "best_bleu = 0\n",
        "early_stopper = EarlyStopping(patience=5)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    teacher_forcing_ratio = 0.5\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP, teacher_forcing_ratio)\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "    bleu = evaluate_bleu(model, val_df, num_samples=5)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | BLEU: {bleu:.4f}\")\n",
        "\n",
        "    if bleu > best_bleu:\n",
        "        best_bleu = bleu\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")  # <- fix here\n",
        "        print(\" Best model saved\")\n",
        "\n",
        "    early_stopper.step(val_loss)  # you are using val_loss to decide stopping\n",
        "    if early_stopper.early_stop:\n",
        "        print(\" Early stopping triggered.\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItTNhC_adLX",
        "outputId": "6f9622d0-f332-4f3c-f706-24274203a4f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\samue\\AppData\\Local\\Temp\\ipykernel_25008\\807416939.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: <unk> ( str1 ) { <unk> <unk> = ( * <unk> <unk> / <unk> <unk> <unk> <unk> <unk> <unk> ; . . ( <unk> , 2 ) ) ; ;\n",
            "Reference: function Conversion ( centi ) { let pixels = ( 96 * centi ) / 2.54 ; document . write ( pixels ) ; return 0 ; }\n"
          ]
        }
      ],
      "source": [
        "load_model(model, \"best_model.pt\")\n",
        "\n",
        "# Try inference\n",
        "test_row = val_df.iloc[0]\n",
        "src_tensor = torch.tensor(test_row['py_input_ids'], dtype=torch.long)\n",
        "predicted_tokens = beam_search(model, src_tensor)\n",
        "print(\"Predicted:\", \" \".join(predicted_tokens))\n",
        "print(\"Reference:\", \" \".join(test_row['js_tokens']))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml2_final",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
