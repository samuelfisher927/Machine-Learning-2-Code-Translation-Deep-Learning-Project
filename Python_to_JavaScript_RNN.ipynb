{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9_8xGjmExCvW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer"
      ],
      "metadata": {
        "id": "WBJHWhcpyNBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, texts, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]):\n",
        "        words = set()\n",
        "        for line in texts:\n",
        "            words.update(line.strip().split())\n",
        "        self.word2idx = {tok: idx for idx, tok in enumerate(special_tokens)}\n",
        "        for word in sorted(words):\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word = {idx: tok for tok, idx in self.word2idx.items()}\n",
        "        self.pad_token_id = self.word2idx[\"<pad>\"]\n",
        "        self.sos_token_id = self.word2idx[\"<sos>\"]\n",
        "        self.eos_token_id = self.word2idx[\"<eos>\"]\n",
        "        self.unk_token_id = self.word2idx[\"<unk>\"]\n",
        "\n",
        "    def encode(self, text, max_len=256):\n",
        "        tokens = [\"<sos>\"] + text.strip().split() + [\"<eos>\"]\n",
        "        token_ids = [self.word2idx.get(tok, self.unk_token_id) for tok in tokens]\n",
        "        token_ids = token_ids[:max_len]\n",
        "        return token_ids + [self.pad_token_id] * (max_len - len(token_ids))\n",
        "\n",
        "    def decode(self, ids):\n",
        "        words = [self.idx2word.get(i, \"<unk>\") for i in ids if i not in (self.pad_token_id, self.eos_token_id, self.sos_token_id)]\n",
        "        return \" \".join(words).strip()"
      ],
      "metadata": {
        "id": "fe_zaWB-xD_9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "qZW2nww1ydOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeTranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer, max_len=256):\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.pairs[idx]\n",
        "        src_ids = self.tokenizer.encode(src, self.max_len)\n",
        "        tgt_ids = self.tokenizer.encode(tgt, self.max_len)\n",
        "        labels = [i if i != self.tokenizer.pad_token_id else -100 for i in tgt_ids]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(src_ids),\n",
        "            \"labels\": torch.tensor(labels),\n",
        "            \"lengths\": (torch.tensor(src_ids) != self.tokenizer.pad_token_id).sum()\n",
        "        }\n"
      ],
      "metadata": {
        "id": "cgCtc5iwxHdE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Model"
      ],
      "metadata": {
        "id": "riKV3tQ2yhV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hid_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        energy = energy.transpose(1, 2)\n",
        "        v = self.v.repeat(hidden.size(0), 1).unsqueeze(1)\n",
        "        attn_weights = torch.bmm(v, energy).squeeze(1)\n",
        "        return torch.softmax(attn_weights, dim=1)\n",
        "\n",
        "class AttnSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hid_dim, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.encoder = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
        "        self.decoder = nn.GRU(emb_dim + hid_dim, hid_dim, batch_first=True)\n",
        "        self.attn = Attention(hid_dim)\n",
        "        self.fc_out = nn.Linear(hid_dim * 2, vocab_size)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, src, trg, lengths):\n",
        "        embedded_src = self.embedding(src)\n",
        "        packed = pack_padded_sequence(embedded_src, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        encoder_outputs, hidden = self.encoder(packed)\n",
        "        encoder_outputs, _ = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
        "\n",
        "        decoder_input = trg.clone()\n",
        "        decoder_input[decoder_input == -100] = self.pad_idx\n",
        "        embedded_trg = self.embedding(decoder_input[:, :-1])\n",
        "        outputs = []\n",
        "        input_step = embedded_trg[:, 0].unsqueeze(1)\n",
        "\n",
        "        for t in range(embedded_trg.size(1)):\n",
        "            a = self.attn(hidden[-1], encoder_outputs).unsqueeze(1)\n",
        "            weighted = torch.bmm(a, encoder_outputs)\n",
        "            rnn_input = torch.cat((input_step, weighted), dim=2)\n",
        "            output, hidden = self.decoder(rnn_input, hidden)\n",
        "            prediction = self.fc_out(torch.cat((output, weighted), dim=2)).squeeze(1)\n",
        "            outputs.append(prediction)\n",
        "            if t + 1 < embedded_trg.size(1):\n",
        "                input_step = embedded_trg[:, t + 1].unsqueeze(1)\n",
        "        return torch.stack(outputs, dim=1)"
      ],
      "metadata": {
        "id": "UYKNz8YDxKWe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "dFoSujRZyk2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        src = batch[\"input_ids\"].to(device)\n",
        "        trg = batch[\"labels\"].to(device)\n",
        "        lengths = batch[\"lengths\"].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            output = model(src, trg, lengths)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "            loss = criterion(output, trg)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(dataloader)\n"
      ],
      "metadata": {
        "id": "JEXazyeYxNeY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evalution"
      ],
      "metadata": {
        "id": "vNWmwsG0yqeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(model, val_pairs, tokenizer, device, max_len=256, num_samples=5):\n",
        "    model.eval()\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    total_bleu = 0\n",
        "    for i in range(min(num_samples, len(val_pairs))):\n",
        "        src_text, tgt_text = val_pairs[i]\n",
        "        src_ids = tokenizer.encode(src_text)\n",
        "        src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
        "        with torch.no_grad(), autocast():\n",
        "            lengths = (src_tensor != tokenizer.pad_token_id).sum(dim=1)\n",
        "            encoder_outputs, hidden = model.encoder(pack_padded_sequence(model.embedding(src_tensor), lengths.cpu(), batch_first=True, enforce_sorted=False))\n",
        "            encoder_outputs, _ = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
        "            token = torch.tensor([[tokenizer.sos_token_id]], device=device)\n",
        "            generated = []\n",
        "            for _ in range(max_len):\n",
        "                embedded = model.embedding(token)\n",
        "                a = model.attn(hidden[-1], encoder_outputs).unsqueeze(1)\n",
        "                weighted = torch.bmm(a, encoder_outputs)\n",
        "                rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "                output, hidden = model.decoder(rnn_input, hidden)\n",
        "                prediction = model.fc_out(torch.cat((output, weighted), dim=2)).squeeze(1)\n",
        "                token = prediction.argmax(dim=1, keepdim=True)\n",
        "                if token.item() == tokenizer.eos_token_id:\n",
        "                    break\n",
        "                generated.append(token.item())\n",
        "                token = token.unsqueeze(1)\n",
        "            pred_text = tokenizer.decode(generated)\n",
        "            bleu = sentence_bleu([tgt_text.split()], pred_text.split(), smoothing_function=smoothie)\n",
        "            print(f\"\\nBLEU: {bleu:.4f}\\nPred: {pred_text}\\nRef : {tgt_text}\")\n",
        "            total_bleu += bleu\n",
        "    return total_bleu / num_samples"
      ],
      "metadata": {
        "id": "JYBh3bD5xP8K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "4CgE3iQkyth9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, path=\"best_model.pt\"):\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_model(model, path=\"best_model.pt\"):\n",
        "    model.load_state_dict(torch.load(path))\n",
        "    model.eval()\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best = float('inf')\n",
        "        self.stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if val_loss < self.best:\n",
        "            self.best = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True"
      ],
      "metadata": {
        "id": "akMHNDlpxSoa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "aiIvlXZiyw7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    def load_pairs(py_file, js_file):\n",
        "        with open(py_file, encoding=\"utf-8\") as f1, open(js_file, encoding=\"utf-8\") as f2:\n",
        "            src_lines = [l.strip() for l in f1 if l.strip()]\n",
        "            tgt_lines = [l.strip() for l in f2 if l.strip()]\n",
        "        return list(zip(src_lines, tgt_lines))\n",
        "\n",
        "    train_pairs = load_pairs(\"train-Python-Javascript-tok.py\", \"train-Python-Javascript-tok.js\")\n",
        "    val_pairs = load_pairs(\"val-Python-Javascript-tok.py\", \"val-Python-Javascript-tok.js\")\n",
        "    all_texts = [src for src, _ in train_pairs + val_pairs] + [tgt for _, tgt in train_pairs + val_pairs]\n",
        "\n",
        "    tokenizer = SimpleTokenizer(all_texts)\n",
        "    VOCAB_SIZE = len(tokenizer.word2idx)\n",
        "\n",
        "    train_ds = CodeTranslationDataset(train_pairs, tokenizer)\n",
        "    val_ds = CodeTranslationDataset(val_pairs, tokenizer)\n",
        "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=16, num_workers=2)\n",
        "\n",
        "    model = AttnSeq2Seq(VOCAB_SIZE, 256, 512, pad_idx=tokenizer.pad_token_id).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    scaler = GradScaler()\n",
        "    early_stopper = EarlyStopping(patience=4)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        print(f\"\\nEpoch {epoch+1}\")\n",
        "        if epoch == 1: model = torch.compile(model)\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, DEVICE, scaler)\n",
        "        bleu = evaluate_bleu(model, val_pairs, tokenizer, DEVICE)\n",
        "        print(f\"Train Loss: {train_loss:.4f} | BLEU: {bleu:.4f}\")\n",
        "        if bleu > 0.5: save_model(model)\n",
        "        early_stopper(train_loss)\n",
        "        if early_stopper.stop: break\n",
        "\n",
        "    load_model(model)\n",
        "    print(\"Final BLEU on validation set:\")\n",
        "    evaluate_bleu(model, val_pairs, tokenizer, DEVICE, num_samples=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTH2mBEjxVL9",
        "outputId": "a8b5f6a9-0419-4e58-e3f8-68c594604157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-baf35ecf793a>:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/734 [00:00<?, ?it/s]<ipython-input-5-a9a6ed6bd2ca>:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Training:  47%|████▋     | 348/734 [04:06<04:33,  1.41it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FVgoIwPTxZMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}